---
description: Generate test-aware task list from Rigour Vibe PRD
globs:
alwaysApply: false
---
# Rule: Rigour Vibe Task Generation

## Goal

To create a detailed, test-driven task list from a Rigour Vibe PRD that ensures quality while maintaining development velocity. Each task includes both implementation and verification requirements.

## Process

1. **Analyze PRD:** Read complexity level and testing requirements from the PRD file
2. **Phase 1: Generate Parent Tasks:** Based on the PRD analysis, create the file and generate the main, high-level tasks required to implement the feature. Use your judgement on how many high-level tasks to use based on complexity level. Present these tasks to the user in the specified format (without sub-tasks yet). Inform the user: "I have generated the high-level tasks based on the PRD. Ready to generate the detailed sub-tasks and testing requirements? Respond with 'Go' to proceed."
3. **Wait for Confirmation:** Pause and wait for the user to respond with "Go"
4. **Phase 2: Generate Detailed Tasks:** Once the user confirms, break down each parent task into smaller, actionable sub-tasks with integrated testing requirements
5. **Define Test Dependencies:** Ensure tests build progressively
6. **Create Quality Checkpoints:** Add verification gates between major tasks
7. **Save Task List:** Store as `YYYY-MM-DD_task_feature-name.md` in `../directives/tasks/` (create directory if needed)

## Output Format

```markdown
# Rigour Vibe Tasks: [Feature Name]

## Project Overview
**Complexity Level:** [From PRD]
**Testing Strategy:** [Summary from PRD]
**Quality Gates:** [Key checkpoints]

## Relevant Files (Following Standardized Project Structure)
- `src/core/[feature]/[component].ts` - Core business logic implementation
- `src/api/[feature]/[endpoint].ts` - API endpoint implementation (if needed)
- `src/ui/[feature]/[component].tsx` - UI component implementation (if needed)
- `src/utils/[feature]/[utility].ts` - Utility functions (if needed)
- `tests/unit/[feature]/[component].test.ts` - Unit tests for core logic
- `tests/integration/[feature]/[workflow].spec.ts` - Integration tests for workflows
- `tests/e2e/[feature]/[scenario].e2e.ts` - End-to-end tests (if needed)
- `tests/fixtures/[feature]/[data].json` - Test data and mock objects
- `directives/tasks/YYYY-MM-DD_task_feature-name.md` - This task breakdown file
- `docs/api/[feature].md` - API documentation (if applicable)
- `environment/docker/Dockerfile` - Docker configuration (if needed)
- `scripts/build/build-[feature].sh` - Build scripts (if needed)

## Progressive Implementation Requirements

### Environment Foundation (Complete First)
- [ ] 0.1 Set up development environment infrastructure
  - [ ] 0.1.1 Create Docker configuration files (if containerization needed)
  - [ ] 0.1.2 Set up environment variables and configuration
  - [ ] 0.1.3 Verify development environment builds and runs
  - [ ] 0.1.4 Establish development workflow scripts

### Test Infrastructure (Complete After Environment)
- [ ] 0.2 Set up test framework within established environment
  - [ ] 0.2.1 Configure test runners for target environment
  - [ ] 0.2.2 Create test utilities and helpers
  - [ ] 0.2.3 Set up test data factories/fixtures
  - [ ] 0.2.4 Verify test pipeline runs in target environment

### Implementation Tasks

- [ ] 1.0 [Parent Task Name]
  - **Tests Required Before Implementation:**
    - [ ] 1.0.T1 Write failing unit tests for [specific functionality]
    - [ ] 1.0.T2 Create integration test stubs for [workflow]
  - **Implementation Subtasks:**
    - [ ] 1.1 [Implementation subtask]
    - [ ] 1.2 [Implementation subtask]
  - **Verification Requirements:**
    - [ ] 1.0.V1 All new tests pass
    - [ ] 1.0.V2 All existing tests still pass
    - [ ] 1.0.V3 Integration points verified
  
- [ ] 2.0 [Parent Task Name]
  - **Tests Required Before Implementation:**
    - [ ] 2.0.T1 Write tests for [functionality]
  - **Implementation Subtasks:**
    - [ ] 2.1 [Implementation subtask]
  - **Verification Requirements:**
    - [ ] 2.0.V1 Full test suite passes
    - [ ] 2.0.V2 No performance regressions

### Quality Gates

#### Gate 1: Development Environment Ready ✅
**Required Before Proceeding to Test Infrastructure**
- [ ] Development environment builds successfully
- [ ] All environment configuration functional
- [ ] Development workflow established
- [ ] No critical environment issues

#### Gate 2: Test Infrastructure Ready ✅
**Required Before Core Feature Implementation**
- [ ] All test frameworks configured and functional
- [ ] Test pipeline runs successfully
- [ ] Test data and fixtures established
- [ ] No critical test setup failures

#### Gate 3: Core Functionality Complete ✅
**Required Before Advanced Features**
- [ ] All core task tests passing
- [ ] Basic integration verified
- [ ] No critical test failures
- [ ] Performance within acceptable range

#### Gate 4: Feature Complete ✅
**Required Before Final Testing**
- [ ] All feature tasks completed
- [ ] Full test coverage achieved
- [ ] Integration tests passing
- [ ] Documentation updated

#### Gate 5: Release Ready ✅
**Required Before Deployment**
- [ ] Complete test suite passing
- [ ] End-to-end scenarios verified
- [ ] Performance benchmarks met
- [ ] Security requirements satisfied
- [ ] Rollback procedures tested

## Test Execution Commands
- **Unit Tests:** `[command for running unit tests]`
- **Integration Tests:** `[command for integration tests]`
- **Full Suite:** `[command for complete test run]`
- **Coverage:** `[command for test coverage report]`

## Definition of Task Completion

A task is only complete when:
1. ✅ All required tests are written and initially failing
2. ✅ Implementation makes tests pass
3. ✅ All previous tests still pass (regression check)
4. ✅ Code meets quality standards
5. ✅ Changes committed with passing CI

## Complexity-Specific Requirements

### Light Touch Tasks
- Focus on happy path testing
- Basic error handling verification
- Smoke tests for integration

### Balanced Rigor Tasks
- Comprehensive unit test coverage
- Edge case handling
- Integration workflow testing
- Error scenario verification

### Full Rigor Tasks
- TDD red-green-refactor cycle
- Exhaustive edge case coverage
- Performance testing
- Security verification
- End-to-end scenario testing
```

## Task Generation Rules

### For Light Touch (Level 1):
- 3-5 main tasks maximum
- 1-2 tests per task
- Focus on core functionality
- Basic integration verification

### For Balanced Rigor (Level 2):
- 5-8 main tasks
- 2-4 tests per task
- Include edge cases
- Integration and error handling tests
- Performance considerations

### For Full Rigor (Level 3):
- 6-12 main tasks
- 3-6 tests per task
- Comprehensive test coverage
- Security and performance tests
- Multiple integration scenarios
- Monitoring and observability

## Phase-Based Generation

1. **Generate High-Level Tasks:** Present main tasks with test requirements
2. **Wait for Approval:** User responds with "Go" to proceed
3. **Detail Subtasks:** Break down each task with specific test and implementation steps
4. **Add Quality Gates:** Insert verification checkpoints
5. **Finalize:** Complete task list with execution commands

## Output Location
- **Format:** Markdown (`.md`)
- **Location:** `../directives/tasks/`
- **Filename:** `YYYY-MM-DD_task_feature-name.md` (where YYYY-MM-DD is today's date)

## File Naming Convention

Use today's date (2025-07-10) and match the feature name from the PRD:
- If PRD is `2025-07-10_prd_user-authentication.md`
- Then task file is `2025-07-10_task_user-authentication.md`

## AI Instructions

1. Analyze the PRD's complexity level first
2. Generate high-level parent tasks appropriate to the rigor level
3. **Environment First:** Always prioritize development environment setup (Docker, configs) before test infrastructure
4. **Test Infrastructure Second:** Set up testing frameworks within the established environment
5. Present parent tasks and wait for user confirmation with "Go" 
6. Only after confirmation, generate detailed subtasks with environment setup BEFORE test requirements
7. Always include environment requirements BEFORE test requirements in detailed tasks
8. Create progressive quality gates starting with environment readiness
9. Ensure each task has clear completion criteria
10. Include specific commands for the established environment (Docker, local, etc.)

## Interaction Model

The process explicitly requires a pause after generating parent tasks to get user confirmation ("Go") before proceeding to generate the detailed sub-tasks with testing requirements. This ensures the high-level plan aligns with user expectations before diving into implementation and testing details.
